{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training\n",
    "Finally, we are ready to work on the __environment__ which determines the training of the RL agents.<br>\n",
    "This is the code contained in construction_env.py, which will be imported to run the training.\n",
    "\n",
    "### 4.1 Action and Observation Spaces\n",
    "__Action Space__<br>\n",
    "Controls the number of the actions and their max. and min. bounds.<br>\n",
    "e.g. If we have 10 RL vehicles, and we want to control their accelerations, we would have an action space of 10, and we could bound their max. and min. accelrations by, say, 1 and -1.\n",
    "e.g. If we want to add lane changes, we'd have a total of 20 actions per iteration.\n",
    "\n",
    "__Observation Space__<br>\n",
    "The vector of information that is read in at each time step and on which is run the training. This function simply specifies what size and bounds this vector is to be, we will write the function that fills it in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/llavezzo/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ray\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "import numpy as np\n",
    "from flow.networks.ring import RingNetwork, ADDITIONAL_NET_PARAMS\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "from flow.core.params import SumoParams, EnvParams, InitialConfig, NetParams\n",
    "from flow.core.params import VehicleParams, SumoCarFollowingParams\n",
    "from flow.controllers import RLController, IDMController, ContinuousRouter\n",
    "from gym.spaces.box import Box\n",
    "from gym.spaces import Tuple\n",
    "from flow.envs import Env\n",
    "from flow.core import rewards\n",
    "\n",
    "\n",
    "ADDITIONAL_ENV_PARAMS = {\n",
    "    \"max_accel\": 2,\n",
    "    \"max_decel\": 2,\n",
    "    \"lane_change_duration\": 5,\n",
    "}\n",
    "\n",
    "MAX_EDGE = 12\n",
    "MAX_LANE = 2\n",
    "observation_edges = [2,3,4,5]\n",
    "\n",
    "class myEnv(Env):\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        num_actions = self.initial_vehicles.num_rl_vehicles\n",
    "        \n",
    "        max_decel = self.env_params.additional_params[\"max_decel\"]\n",
    "        max_accel = self.env_params.additional_params[\"max_accel\"]\n",
    "\n",
    "        lb = [-abs(max_decel), -1] * num_actions\n",
    "        ub = [max_accel, 1] * num_actions\n",
    "\n",
    "        return Box(np.array(lb), np.array(ub), dtype=np.float32)\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            shape=(10*8+4,),\n",
    "            dtype=np.float32\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Apply Actions\n",
    "At each time step, we want to apply certain actions.<br>\n",
    "Here, we apply an __acceleration__ and a __lane change__ to each of the RL vehicles.<br>\n",
    "In the implementation, we see that vehicles that changed lanes recently aren't allowed to do so again for a certain time 'lane_change_duration', and vehicles in the junction right before the bottleneck of the construction zone aren't allowed to change lanes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _apply_rl_actions(self, rl_actions):\n",
    "    num_rl = self.k.vehicle.num_rl_vehicles\n",
    "    rl_ids = self.k.vehicle.get_rl_ids()\n",
    "    sorted_rl_ids = sorted(self.k.vehicle.get_rl_ids(),\n",
    "                           key=self.k.vehicle.get_x_by_id)\n",
    "\n",
    "    acceleration = rl_actions[::2][:num_rl]\n",
    "    direction = np.round(rl_actions[1::2])[:num_rl]\n",
    "\n",
    "    # represents vehicles that are allowed to change lanes\n",
    "    non_lane_changing_veh = [\n",
    "        self.time_counter <= self.env_params.additional_params[\n",
    "            'lane_change_duration'] + self.k.vehicle.get_last_lc(veh_id)\n",
    "        for veh_id in sorted_rl_ids]\n",
    "\n",
    "    #Check that vehicles in junction before the constructione zone\n",
    "    # don't change lanes\n",
    "    for i, veh_id in enumerate(sorted_rl_ids):\n",
    "        edge_num = self.k.vehicle.get_edge(veh_id)\n",
    "        if(edge_num == '' or edge_num[0] == ':'):\n",
    "            direction[i] = 0\n",
    "        if(edge_num == ':gneJ6'):\n",
    "            direction[i] = 0\n",
    "\n",
    "    # vehicle that are not allowed to change have their directions set to 0\n",
    "    direction[non_lane_changing_veh] = \\\n",
    "        np.array([0] * sum(non_lane_changing_veh))\n",
    "\n",
    "    self.k.vehicle.apply_acceleration(sorted_rl_ids, acc=acceleration)\n",
    "    self.k.vehicle.apply_lane_change(sorted_rl_ids, direction=direction)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Get State\n",
    "At each time step, we are interested in obtaining information about the network and the vehicles in it, which is then used to train the RL agents.<br>\n",
    "To do so, we use get_state, which returns a vector of information whose size and bounds are specified by observation_space.\n",
    "\n",
    "We collect the following:\n",
    "- Position, Velocity, Edge, and Lane of all RL vehicles in the network.\n",
    "- The relative position, speed, and type (i.e. human or RL) of the vehicle in front of and behind of every RL vehicle.\n",
    "- The density of vehicles and the average speed in each lane in the highway prior to the construction zone.\n",
    "\n",
    "One could feed all the positions and speeds of all the cars in the network, though that would be a really large observation space, which will make the training much longer and may not be as relevant as some of the information provided through this environment. Furthermore, the number of cars in the network is changing, as some are spawned at the inflow, and some disappear at the end of the highway. Therefore, one would have to initialize a very large initial observation space, fill it with all this info, and then pad it with 0's to fill it up completely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state(self, **kwargs):\n",
    "        \n",
    "        ids = self.k.vehicle.get_ids()\n",
    "        rl_ids = self.k.vehicle.get_rl_ids()\n",
    "        \n",
    "        #normalizing constants\n",
    "        max_speed = self.k.network.max_speed()\n",
    "        for veh_id in ids:\n",
    "            if(abs(self.k.vehicle.get_speed(veh_id)) > 10000): continue\n",
    "            if(self.k.vehicle.get_speed(veh_id) > max_speed): max_speed = self.k.vehicle.get_speed(veh_id)\n",
    "        max_length = self.k.network.length()\n",
    "        \n",
    "        #rl vehicle info (size 4*num_rl)\n",
    "        pos=[]  \n",
    "        vel=[]  \n",
    "        edges=[] \n",
    "        lanes=[] \n",
    "\n",
    "        #follower/leader info (size 6*num_rl)\n",
    "        types=[]\n",
    "        follower_dv = []\n",
    "        follower_dx = []\n",
    "        leader_dv = []\n",
    "        leader_dx = []\n",
    "\n",
    "        #network info (2*2)\n",
    "        lane_traffic=[0,0] \n",
    "        lane_traffic_speed=[0,0]\n",
    "\n",
    "\n",
    "        #Density and average speed of vehicles in lane before construction zone\n",
    "        for veh_id in ids:\n",
    "\n",
    "            edge_num = self.k.vehicle.get_edge(veh_id)\n",
    "            lane_num = int(self.k.vehicle.get_lane(veh_id))\n",
    "            v = self.k.vehicle.get_speed(veh_id)\n",
    "            if(-1 <= v/max_speed <= 1):\n",
    "                if(edge_num == \"edge3\"):\n",
    "                    lane_traffic[lane_num] += 1\n",
    "                    lane_traffic_speed[lane_num] += v/max_speed\n",
    "                \n",
    "\n",
    "        #Info for each RL vehicle\n",
    "        for rl_id in rl_ids:\n",
    "            \n",
    "            edge_num = self.k.vehicle.get_edge(rl_id)\n",
    "            if edge_num is None or edge_num == '' or edge_num[0] == ':':\n",
    "                edge_num = -1\n",
    "            else:\n",
    "                edge_num = edge_num[4:]\n",
    "            lane_num = self.k.vehicle.get_lane(rl_id)\n",
    "            r = self.k.vehicle.get_x_by_id(rl_id)\n",
    "            v = self.k.vehicle.get_speed(rl_id)\n",
    "            \n",
    "            follower_id = self.k.vehicle.get_follower(rl_id)\n",
    "            leader_id = self.k.vehicle.get_leader(rl_id)\n",
    "\n",
    "            if leader_id in [\"\", None]:\n",
    "                leader_dv.append(1)\n",
    "                leader_dx.append(1)\n",
    "                types.append(0)\n",
    "            else:\n",
    "                leader_pos = (self.k.vehicle.get_x_by_id(leader_id) - self.k.vehicle.get_x_by_id(rl_id))/max_length\n",
    "                leader_speed = self.k.vehicle.get_speed(leader_id)/max_speed\n",
    "\n",
    "                if -1 <= leader_speed <= 1:\n",
    "                    leader_dv.append(leader_speed)\n",
    "                else: \n",
    "                    leader_dv.append(0)\n",
    "\n",
    "                if -1 <= leader_pos <= 1:\n",
    "                    leader_dx.append(leader_pos)\n",
    "                else: \n",
    "                    leader_dx.append(0)\n",
    "\n",
    "                if(rl_id in rl_ids): types.append(1)\n",
    "                else: types.append(-1)\n",
    "\n",
    "            if follower_id in [\"\",None]:\n",
    "                follower_dv.append(1)\n",
    "                follower_dx.append(1)\n",
    "                types.append(0)\n",
    "            else:\n",
    "                follower_pos = (self.k.vehicle.get_x_by_id(rl_id) - self.k.vehicle.get_x_by_id(follower_id))/max_length\n",
    "                follower_speed = self.k.vehicle.get_speed(follower_id)/max_speed\n",
    "\n",
    "                if -1 <= follower_speed <= 1:\n",
    "                    follower_dv.append(follower_speed)\n",
    "                else: \n",
    "                    follower_dv.append(0)\n",
    "                 \n",
    "                if -1 <= follower_pos <= 1:\n",
    "                    follower_dx.append(follower_pos)\n",
    "                else: \n",
    "                    follower_dx.append(0)\n",
    "\n",
    "                if(rl_id in rl_ids): types.append(1)\n",
    "                else: types.append(-1)\n",
    "\n",
    "\n",
    "            edge_num = int(edge_num)/MAX_EDGE\n",
    "            lane_num = int(lane_num)/MAX_LANE\n",
    "            if -1 <= edge_num <= 1:\n",
    "                edges.append(edge_num)\n",
    "            else: \n",
    "                edge.append(0)\n",
    "            if -1 <= lane_num <= 1:\n",
    "                lanes.append(lane_num)\n",
    "            else: \n",
    "                lanes.append(0)\n",
    "\n",
    "            r = r/max_length\n",
    "            v = v/max_speed\n",
    "            \n",
    "            if -1 <= r <= 1:\n",
    "                pos.append(r)\n",
    "            else:\n",
    "                pos.append(0)\n",
    "\n",
    "            if -1 <= v <= 1:\n",
    "                vel.append(v)\n",
    "            else:\n",
    "                vel.append(0)\n",
    "\n",
    "        #Normalize\n",
    "        for i in range(2):\n",
    "            if(lane_traffic[i] != 0): lane_traffic_speed[i] = (lane_traffic_speed[i]/lane_traffic[i])\n",
    "            else: lane_traffic_speed[i] = 0\n",
    "            lane_traffic[i] = lane_traffic[i] / len(ids)\n",
    "\n",
    "        state = np.concatenate((pos,vel,edges,lanes,\n",
    "                                types,follower_dx,follower_dv,leader_dv,leader_dx,\n",
    "                                lane_traffic,lane_traffic_speed))\n",
    "                    \n",
    "        return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 The Reward Function\n",
    "We are now collecting the state of the network and its vehicles at each time step and we defined the possible actions that the Rl agents can take, we need to define a __reward function__ that the training will be based on.<br>\n",
    "This is another place where one can affect the training by prioritizing certain behvaiovrs and punishing others.<br>\n",
    "Three specific features are taken into account in this reward function:\n",
    "1. Mean speed of the human vehicles in the edges preceding the construction zone.\n",
    "2. The acceleration of the RL agents.\n",
    "3. The number of vehicles not moving.\n",
    "\n",
    "The inclusion of the first parameter is to encourage the increase of the mean velocity of the human cars on the highway where the construction zone causes the slowdown. Since the RL agents will be rewarded based on the magnitude of this number, they will try to increase it.<br>\n",
    "The acceleration of the RL agents is also rewarded because we want them to be driving as fast as possible when they can. This encourages them to keep going if they have the space to.<br>\n",
    "The penalization of the standstill similiarly punsishes the training by returning a negative number calculated by the number of vehicles that aren't moving in the edges before the consturction zone.<br>\n",
    "\n",
    "Each of the parameters are multiplied by a specific gain, and everything is normalized to be in (0,1). By changing the gains we can control how each parameter affects the training.\n",
    "\n",
    "Some other sample reward functions are included in /flow/core/rewards.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(self, rl_actions, **kwargs):\n",
    "    ids = self.k.vehicle.get_ids()\n",
    "    speeds = self.k.vehicle.get_speed(ids)\n",
    "    rl_ids = self.k.vehicle.get_rl_ids()\n",
    "\n",
    "    #Only count speeds of human cars in edge prior to the 'construction site'\n",
    "    #RL vehicles encouraged to make forward progress\n",
    "    meanHumanSpeed = 0\n",
    "    meanRLSpeed = 0\n",
    "    RL_SPEED_GAIN = 0.1\n",
    "    humanSpeeds = []\n",
    "    rlSpeeds = []\n",
    "    for veh_id in ids: \n",
    "        if veh_id in rl_ids:\n",
    "            speed = self.k.vehicle.get_speed(veh_id)\n",
    "            if abs(speed) > 10000: continue\n",
    "            rlSpeeds.append(abs(speed))\n",
    "        else:\n",
    "            edge = self.k.vehicle.get_edge(veh_id)\n",
    "            if edge == \"edge3\" or edge == \"edge4\":\n",
    "                speed = self.k.vehicle.get_speed(veh_id)\n",
    "                if abs(speed) > 10000: continue\n",
    "                humanSpeeds.append(abs(speed))\n",
    "\n",
    "    if(len(humanSpeeds)==0): meanHumanSpeed = 0\n",
    "    else: meanHumanSpeed = np.mean(humanSpeeds)\n",
    "\n",
    "    if(len(rlSpeeds)==0): meanRLSpeed = 0\n",
    "    else: meanRLSpeed = np.mean(rlSpeeds)*RL_SPEED_GAIN\n",
    "\n",
    "    return (meanHumanSpeed + meanRLSpeed - abs(rewards.penalize_standstill(self,gain=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Putting it all togheter\n",
    "Define the environment, and run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import ray\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "import numpy as np\n",
    "from flow.networks.ring import RingNetwork, ADDITIONAL_NET_PARAMS\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "from flow.core.params import SumoParams, EnvParams, InitialConfig, NetParams\n",
    "from flow.core.params import VehicleParams, SumoCarFollowingParams\n",
    "from flow.controllers import RLController, IDMController, ContinuousRouter\n",
    "from gym.spaces.box import Box\n",
    "from gym.spaces import Tuple\n",
    "from flow.envs import Env\n",
    "from flow.core import rewards\n",
    "\n",
    "\n",
    "ADDITIONAL_ENV_PARAMS = {\n",
    "    \"max_accel\": 2,\n",
    "    \"max_decel\": 2,\n",
    "    \"lane_change_duration\": 5,\n",
    "}\n",
    "\n",
    "MAX_EDGE = 12\n",
    "MAX_LANE = 2\n",
    "observation_edges = [2,3,4,5]\n",
    "\n",
    "class myEnv(Env):\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        num_actions = self.initial_vehicles.num_rl_vehicles*2\n",
    "        \n",
    "        max_decel = self.env_params.additional_params[\"max_decel\"]\n",
    "        max_accel = self.env_params.additional_params[\"max_accel\"]\n",
    "\n",
    "        lb = [-abs(max_decel), -1] * num_actions \n",
    "        ub = [max_accel, 1] * num_actions \n",
    "\n",
    "        return Box(np.array(lb), np.array(ub), dtype=np.float32)\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return Box(\n",
    "            low=-1,\n",
    "            high=1,\n",
    "            #shape=(2*self.initial_vehicles.num_vehicles,),\n",
    "            shape=(10*8+4,),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def _apply_rl_actions(self, rl_actions):\n",
    "        num_rl = self.k.vehicle.num_rl_vehicles\n",
    "        rl_ids = self.k.vehicle.get_rl_ids()\n",
    "        sorted_rl_ids = sorted(self.k.vehicle.get_rl_ids(),\n",
    "                               key=self.k.vehicle.get_x_by_id)\n",
    "        \n",
    "        acceleration = rl_actions[::2][:num_rl]\n",
    "        direction = np.round(rl_actions[1::2])[:num_rl]\n",
    "\n",
    "        # represents vehicles that are allowed to change lanes\n",
    "        non_lane_changing_veh = [\n",
    "            self.time_counter <= self.env_params.additional_params[\n",
    "                'lane_change_duration'] + self.k.vehicle.get_last_lc(veh_id)\n",
    "            for veh_id in sorted_rl_ids]\n",
    "        \n",
    "        #Check that vehicles in junction begore edge with\n",
    "        # 2 lanes don't try to change to lane 3\n",
    "        for i, veh_id in enumerate(sorted_rl_ids):\n",
    "            edge_num = self.k.vehicle.get_edge(veh_id)\n",
    "            if(edge_num == '' or edge_num[0] == ':'):\n",
    "                direction[i] = 0\n",
    "            if(edge_num == ':gneJ6'):\n",
    "                direction[i] = 0\n",
    "\n",
    "        # vehicle that are not allowed to change have their directions set to 0\n",
    "        direction[non_lane_changing_veh] = \\\n",
    "            np.array([0] * sum(non_lane_changing_veh))\n",
    "\n",
    "        self.k.vehicle.apply_acceleration(sorted_rl_ids, acc=acceleration)\n",
    "        self.k.vehicle.apply_lane_change(sorted_rl_ids, direction=direction)\n",
    "\n",
    "    def get_state(self, **kwargs):\n",
    "        \n",
    "        ids = self.k.vehicle.get_ids()\n",
    "        rl_ids = self.k.vehicle.get_rl_ids()\n",
    "        \n",
    "        #normalizing constants\n",
    "        max_speed = self.k.network.max_speed()\n",
    "        for veh_id in ids:\n",
    "            if(abs(self.k.vehicle.get_speed(veh_id)) > 10000): continue\n",
    "            if(self.k.vehicle.get_speed(veh_id) > max_speed): max_speed = self.k.vehicle.get_speed(veh_id)\n",
    "        max_length = self.k.network.length()\n",
    "        \n",
    "        #rl vehicle info (4*num_rl)\n",
    "        pos=[]  \n",
    "        vel=[]  \n",
    "        edges=[] \n",
    "        lanes=[] \n",
    "\n",
    "        #follower/leader info (size 6*num_rl)\n",
    "        types=[]\n",
    "        follower_dv = []\n",
    "        follower_dx = []\n",
    "        leader_dv = []\n",
    "        leader_dx = []\n",
    "\n",
    "        #network info (2*2\n",
    "        lane_traffic=[0,0] \n",
    "        lane_traffic_speed=[0,0]\n",
    "\n",
    "\n",
    "        #Density and average speed of vehicles in lane before construction zone\n",
    "        for veh_id in ids:\n",
    "\n",
    "            edge_num = self.k.vehicle.get_edge(veh_id)\n",
    "            lane_num = int(self.k.vehicle.get_lane(veh_id))\n",
    "            v = self.k.vehicle.get_speed(veh_id)\n",
    "            if(-1 <= v/max_speed <= 1):\n",
    "                if(edge_num == \"edge3\"):\n",
    "                    lane_traffic[lane_num] += 1\n",
    "                    lane_traffic_speed[lane_num] += v/max_speed\n",
    "                \n",
    "\n",
    "        #Info for each RL vehicle\n",
    "        for rl_id in rl_ids:\n",
    "            \n",
    "            edge_num = self.k.vehicle.get_edge(rl_id)\n",
    "            if edge_num is None or edge_num == '' or edge_num[0] == ':':\n",
    "                edge_num = -1\n",
    "            else:\n",
    "                edge_num = edge_num[4:]\n",
    "            lane_num = self.k.vehicle.get_lane(rl_id)\n",
    "            r = self.k.vehicle.get_x_by_id(rl_id)\n",
    "            v = self.k.vehicle.get_speed(rl_id)\n",
    "            \n",
    "            follower_id = self.k.vehicle.get_follower(rl_id)\n",
    "            leader_id = self.k.vehicle.get_leader(rl_id)\n",
    "\n",
    "            if leader_id in [\"\", None]:\n",
    "                leader_dv.append(1)\n",
    "                leader_dx.append(1)\n",
    "                types.append(0)\n",
    "            else:\n",
    "                leader_pos = (self.k.vehicle.get_x_by_id(leader_id) - self.k.vehicle.get_x_by_id(rl_id))/max_length\n",
    "                leader_speed = self.k.vehicle.get_speed(leader_id)/max_speed\n",
    "\n",
    "                if -1 <= leader_speed <= 1:\n",
    "                    leader_dv.append(leader_speed)\n",
    "                else: \n",
    "                    #print(\"VALUE ERROR LEADER_DV: OUTSIDE RANGE\", self.k.vehicle.get_speed(leader_id), leader_id)\n",
    "                    leader_dv.append(0)\n",
    "\n",
    "                if -1 <= leader_pos <= 1:\n",
    "                    leader_dx.append(leader_pos)\n",
    "                else: \n",
    "                    #print(\"VALUE ERROR LEADER_DX: OUTSIDE RANGE\", leader_speed, leader_id)\n",
    "                    leader_dx.append(0)\n",
    "\n",
    "                if(rl_id in rl_ids): types.append(1)\n",
    "                else: types.append(-1)\n",
    "\n",
    "            if follower_id in [\"\",None]:\n",
    "                follower_dv.append(1)\n",
    "                follower_dx.append(1)\n",
    "                types.append(0)\n",
    "            else:\n",
    "                follower_pos = (self.k.vehicle.get_x_by_id(rl_id) - self.k.vehicle.get_x_by_id(follower_id))/max_length\n",
    "                follower_speed = self.k.vehicle.get_speed(follower_id)/max_speed\n",
    "\n",
    "                if -1 <= follower_speed <= 1:\n",
    "                    follower_dv.append(follower_speed)\n",
    "                else: \n",
    "                    #print(\"VALUE ERROR FOLLOWER_DV: OUTSIDE RANGE\", self.k.vehicle.get_speed(follower_id), follower_id)\n",
    "                    follower_dv.append(0)\n",
    "                 \n",
    "                if -1 <= follower_pos <= 1:\n",
    "                    follower_dx.append(follower_pos)\n",
    "                else: \n",
    "                    #print(\"VALUE ERROR FOLLOWER_DV: OUTSIDE RANGE\", self.k.vehicle.get_speed(follower_id), follower_id)\n",
    "                    follower_dx.append(0)\n",
    "\n",
    "                if(rl_id in rl_ids): types.append(1)\n",
    "                else: types.append(-1)\n",
    "\n",
    "\n",
    "            edge_num = int(edge_num)/MAX_EDGE\n",
    "            lane_num = int(lane_num)/MAX_LANE\n",
    "            if -1 <= edge_num <= 1:\n",
    "                edges.append(edge_num)\n",
    "            else: \n",
    "                #print(\"VALUE ERROR EDGE: OUTSIDE RANGE\", edge_num)\n",
    "                edge.append(0)\n",
    "            if -1 <= lane_num <= 1:\n",
    "                lanes.append(lane_num)\n",
    "            else: \n",
    "                #print(\"VALUE ERROR LANE: OUTSIDE RANGE\", lane_num)\n",
    "                lanes.append(0)\n",
    "\n",
    "            r = r/max_length\n",
    "            v = v/max_speed\n",
    "            \n",
    "            if -1 <= r <= 1:\n",
    "                pos.append(r)\n",
    "            else:\n",
    "                pos.append(0)\n",
    "\n",
    "            if -1 <= v <= 1:\n",
    "                vel.append(v)\n",
    "            else:\n",
    "                vel.append(0)\n",
    "\n",
    "\n",
    "        for i in range(2):\n",
    "            if(lane_traffic[i] != 0): lane_traffic_speed[i] = (lane_traffic_speed[i]/lane_traffic[i])\n",
    "            else: lane_traffic_speed[i] = 0\n",
    "            lane_traffic[i] = lane_traffic[i] / len(ids)\n",
    "\n",
    "        state = np.concatenate((pos,vel,edges,lanes,\n",
    "                                types,follower_dx,follower_dv,leader_dv,leader_dx,\n",
    "                                lane_traffic,lane_traffic_speed))\n",
    "        \n",
    "        #if(np.max(state) > 1 or np.min(state) < -1 or len(state) != 108): print(\"ERROR IN STATE\")\n",
    "            \n",
    "        return state\n",
    "\n",
    "    def compute_reward(self, rl_actions, **kwargs):\n",
    "        ids = self.k.vehicle.get_ids()\n",
    "        speeds = self.k.vehicle.get_speed(ids)\n",
    "        rl_ids = self.k.vehicle.get_rl_ids()\n",
    "\n",
    "        #Only count speeds of human cars in edge prior to the 'construction site'\n",
    "        #RL vehicles encouraged to make forward progress\n",
    "        meanHumanSpeed = 0\n",
    "        meanRLSpeed = 0\n",
    "        RL_SPEED_GAIN = 0.1\n",
    "        humanSpeeds = []\n",
    "        rlSpeeds = []\n",
    "        for veh_id in ids: \n",
    "            if veh_id in rl_ids:\n",
    "                speed = self.k.vehicle.get_speed(veh_id)\n",
    "                if abs(speed) > 10000: continue\n",
    "                rlSpeeds.append(abs(speed))\n",
    "            else:\n",
    "                edge = self.k.vehicle.get_edge(veh_id)\n",
    "                if edge == \"edge3\" or edge == \"edge4\":\n",
    "                    speed = self.k.vehicle.get_speed(veh_id)\n",
    "                    if abs(speed) > 10000: continue\n",
    "                    humanSpeeds.append(abs(speed))\n",
    "\n",
    "        if(len(humanSpeeds)==0): meanHumanSpeed = 0\n",
    "        else: meanHumanSpeed = np.mean(humanSpeeds)\n",
    "\n",
    "        if(len(rlSpeeds)==0): meanRLSpeed = 0\n",
    "        else: meanRLSpeed = np.mean(rlSpeeds)*RL_SPEED_GAIN\n",
    "            \n",
    "        return (meanHumanSpeed + meanRLSpeed - abs(rewards.penalize_standstill(self,gain=0.3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the training, we import the environment as was outlined above, which is included in the file constructionEnv.py.<br>\n",
    "The rest of the code has the same setup as the simulation in the previous part of the tutorial (that's why we went through it).<br>\n",
    "The RL vehicle class is given the RLController, which is guided by the environment we wrote. This will supercede any action by SUMO on the RL agent that would conflict with the apply_rl_actions function of the environment.<br>\n",
    "\n",
    "It's better to submit this as a Python script, and you can make use of multiple CPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import ray\n",
    "import numpy as np\n",
    "from ray.rllib.agents.registry import get_agent_class\n",
    "from ray.tune import run_experiments\n",
    "from ray.tune.registry import register_env\n",
    "from flow.networks import Network\n",
    "from flow.networks.ring import RingNetwork, ADDITIONAL_NET_PARAMS\n",
    "from flow.utils.registry import make_create_env\n",
    "from flow.utils.rllib import FlowParamsEncoder\n",
    "from flow.core.params import SumoParams, EnvParams, InitialConfig, NetParams, VehicleParams, SumoCarFollowingParams, InFlows, SumoLaneChangeParams\n",
    "from flow.controllers import RLController, IDMController, SimLaneChangeController\n",
    "from gym.spaces.box import Box\n",
    "from gym.spaces import Tuple\n",
    "\n",
    "from constructionRouter import ConstructionRouter\n",
    "from constructionEnv_simplifiedV2 import myEnv\n",
    "\n",
    "# time horizon of a single rollout\n",
    "HORIZON = 5000 #5000\n",
    "# number of rollouts per training iteration\n",
    "N_ROLLOUTS = 10\n",
    "# number of parallel workers\n",
    "N_CPUS = 1\n",
    "\n",
    "vehicles = VehicleParams()\n",
    "vehicles.add(\"rl\",\n",
    "             acceleration_controller=(RLController, {}),\n",
    "             lane_change_controller=(SimLaneChangeController, {}),\n",
    "             routing_controller=(ConstructionRouter, {}),\n",
    "             car_following_params=SumoCarFollowingParams(\n",
    "                 speed_mode=\"obey_safe_speed\",  \n",
    "             ),\n",
    "             num_vehicles=8\n",
    "             )\n",
    "vehicles.add(\"human\",\n",
    "             acceleration_controller=(IDMController, {}),\n",
    "             lane_change_controller=(SimLaneChangeController, {}),\n",
    "             car_following_params=SumoCarFollowingParams(\n",
    "                 speed_mode=25\n",
    "             ),\n",
    "             lane_change_params = SumoLaneChangeParams(lane_change_mode=1621),\n",
    "             num_vehicles=0)\n",
    "\n",
    "# specify the edges vehicles can originate on\n",
    "initial_config = InitialConfig(\n",
    "    edges_distribution=[\"edge4\"]\n",
    ")\n",
    "\n",
    "# specify the routes for vehicles in the network\n",
    "class Network(Network):\n",
    "\n",
    "    def specify_routes(self, net_params):\n",
    "        return {\n",
    "                \"edge1\": [\"edge1\",\"edge2\",\"edge3\",\"edge4\",\"edge5\",\"edge6\"],\n",
    "                #\"edge3\": [\"edge3\",\"edge4\",\"edge5\",\"edge10\",\"edge11\",\"edge12\",\"edge3\"],\n",
    "                \"edge4\": [\"edge4\",\"edge5\",\"edge10\",\"edge11\",\"edge12\",\"edge3\",\"edge4\"],\n",
    "                }\n",
    "\n",
    "\n",
    "inflow = InFlows()\n",
    "inflow.add(veh_type=\"human\",\n",
    "           edge=\"edge1\",\n",
    "           vehs_per_hour=3000,\n",
    "            depart_lane=\"random\",\n",
    "            depart_speed=\"random\",\n",
    "            color=\"white\")\n",
    "\n",
    "file_dir = \"/home/llavezzo/\"\n",
    "net_params = NetParams(\n",
    "    template=\"/mnt/c/users/llave/Documents/GitHub/flow_osuphysics/lucalavezzo/construction/constructionV6.net.xml\",\n",
    "    inflows=inflow\n",
    ")\n",
    "\n",
    "flow_params = dict(\n",
    "    # name of the experiment\n",
    "    exp_tag=\"construction_traffic_simplified\",\n",
    "\n",
    "    # name of the flow environment the experiment is running on\n",
    "    env_name=myEnv,  # <------ here we replace the environment with our new environment\n",
    "\n",
    "    # name of the network class the experiment is running on\n",
    "    network=Network,\n",
    "\n",
    "    # simulator that is used by the experiment\n",
    "    simulator='traci',\n",
    "\n",
    "    # sumo-related parameters (see flow.core.params.SumoParams)\n",
    "    sim=SumoParams(\n",
    "        sim_step=0.5,\n",
    "        render=False,\n",
    "        restart_instance=True,\n",
    "    ),\n",
    "\n",
    "    # environment related parameters (see flow.core.params.EnvParams)\n",
    "    env=EnvParams(\n",
    "        horizon=HORIZON,\n",
    "        warmup_steps=1000,\n",
    "        clip_actions=False,\n",
    "        additional_params={\n",
    "            \"target_velocity\": 50,\n",
    "            \"sort_vehicles\": False,\n",
    "            \"max_accel\": 2,\n",
    "            \"max_decel\": 2,\n",
    "            \"lane_change_duration\": 5,\n",
    "        },\n",
    "    ),\n",
    "\n",
    "    # network-related parameters (see flow.core.params.NetParams and the\n",
    "    # network's documentation or ADDITIONAL_NET_PARAMS component)\n",
    "    net=net_params,\n",
    "\n",
    "    # vehicles to be placed in the network at the start of a rollout (see\n",
    "    # flow.core.params.VehicleParams)\n",
    "    veh=vehicles,\n",
    "\n",
    "    # parameters specifying the positioning of vehicles upon initialization/\n",
    "    # reset (see flow.core.params.InitialConfig)\n",
    "    initial=initial_config\n",
    ")\n",
    "\n",
    "\n",
    "def setup_exps():\n",
    "    \"\"\"Return the relevant components of an RLlib experiment.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        name of the training algorithm\n",
    "    str\n",
    "        name of the gym environment to be trained\n",
    "    dict\n",
    "        training configuration parameters\n",
    "    \"\"\"\n",
    "    alg_run = \"PPO\"\n",
    "\n",
    "    agent_cls = get_agent_class(alg_run)\n",
    "    config = agent_cls._default_config.copy()\n",
    "    config[\"num_workers\"] = N_CPUS\n",
    "    config[\"train_batch_size\"] = HORIZON * N_ROLLOUTS\n",
    "    config[\"gamma\"] = 0.999  # discount rate\n",
    "    config[\"model\"].update({\"fcnet_hiddens\": [32, 32, 32, 32]})\n",
    "    config[\"use_gae\"] = True\n",
    "    config[\"lambda\"] = 0.97\n",
    "    config[\"kl_target\"] = 0.02\n",
    "    config[\"num_sgd_iter\"] = 10\n",
    "    config['clip_actions'] = False  # FIXME(ev) temporary ray bug\n",
    "    config[\"horizon\"] = HORIZON\n",
    "\n",
    "    # save the flow params for replay\n",
    "    flow_json = json.dumps(\n",
    "        flow_params, cls=FlowParamsEncoder, sort_keys=True, indent=4)\n",
    "    config['env_config']['flow_params'] = flow_json\n",
    "    config['env_config']['run'] = alg_run\n",
    "\n",
    "    create_env, gym_name = make_create_env(params=flow_params, version=0)\n",
    "\n",
    "    # Register as rllib env\n",
    "    register_env(gym_name, create_env)\n",
    "    return alg_run, gym_name, config\n",
    "\n",
    "alg_run, gym_name, config = setup_exps()\n",
    "ray.init(num_cpus=N_CPUS + 1)\n",
    "trials = run_experiments({\n",
    "    flow_params[\"exp_tag\"]: {\n",
    "        \"run\": alg_run,\n",
    "        \"env\": gym_name,\n",
    "        \"config\": {\n",
    "            **config\n",
    "        },\n",
    "        \"checkpoint_freq\": 5,\n",
    "        \"checkpoint_at_end\": True,\n",
    "        \"max_failures\": 999,\n",
    "        \"stop\": {\n",
    "            \"training_iteration\": 200,\n",
    "        },\n",
    "    }\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (flow)",
   "language": "python",
   "name": "flow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
